{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型选择、欠拟合和过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练误差（training error）和泛化误差（generalization error）**\n",
    "\n",
    "通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。\n",
    "\n",
    "一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。\n",
    "\n",
    "机器学习模型应关注降低泛化误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**验证数据集**\n",
    "\n",
    "从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集$（validation set）$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K折交叉验证**\n",
    "\n",
    "把集合划为$K$个子集合，训练$K$次，每次使用$1$个集合作为验证集另外$k-1$个集合作为训练集。最后，我们对这$K$次训练误差和验证误差分别求平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**欠拟合和过拟合**\n",
    "\n",
    "接下来，我们将探究模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合$（underfitting）$；另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合$（overfitting）$。在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。\n",
    "\n",
    "给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。\n",
    "\n",
    "一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**多项式函数拟合实验**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5\n",
    "features = torch.randn((n_train + n_test, 1))\n",
    "poly_features = torch.cat((features, torch.pow(features, 2), torch.pow(features, 3)), 1) \n",
    "labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]\n",
    "          + true_w[2] * poly_features[:, 2] + true_b)\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7292],\n",
       "         [ 1.3732]]), tensor([[-1.7292,  2.9902, -5.1708],\n",
       "         [ 1.3732,  1.8856,  2.5892]]), tensor([-36.1909,  14.7375]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:2], poly_features[:2], labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多项式函数训练与拟合\n",
    "num_epochs, loss = 100, torch.nn.MSELoss()\n",
    "import d2lzh_pytorch as d2l\n",
    "def fit_and_plot(train_features, test_features, train_labels, test_labels):\n",
    "    net = torch.nn.Linear(train_features.shape[-1], 1)\n",
    "    # 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了\n",
    "\n",
    "    batch_size = min(10, train_labels.shape[0])    \n",
    "    dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    train_ls, test_ls = [], []\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            l = loss(net(X), y.view(-1, 1))\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        train_labels = train_labels.view(-1, 1)\n",
    "        test_labels = test_labels.view(-1, 1)\n",
    "        train_ls.append(loss(net(train_features), train_labels).item())\n",
    "        test_ls.append(loss(net(test_features), test_labels).item())\n",
    "    print('final epoch: train loss', train_ls[-1], 'test loss', test_ls[-1])\n",
    "    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n",
    "             range(1, num_epochs + 1), test_ls, ['train', 'test'])\n",
    "    print('weight:', net.weight.data,\n",
    "          '\\nbias:', net.bias.data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**三阶多项式函数拟合正常**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final epoch: train loss 0.0001095805928343907 test loss 8.90009177965112e-05\n",
      "weight: tensor([[ 1.2018, -3.3980,  5.5990]]) \n",
      "bias: tensor([4.9963])\n"
     ]
    }
   ],
   "source": [
    "fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], \n",
    "            labels[:n_train], labels[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**线性函数拟合(欠拟合)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final epoch: train loss 131.29928588867188 test loss 170.78704833984375\n",
      "weight: tensor([[17.2907]]) \n",
      "bias: tensor([0.8283])\n"
     ]
    }
   ],
   "source": [
    "fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train],\n",
    "             labels[n_train:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练样本不足（过拟合）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final epoch: train loss 2.0668267097789794e-09 test loss 19.742883682250977\n",
      "weight: tensor([[ 1.6982, -1.1243,  5.7240]]) \n",
      "bias: tensor([-0.2948])\n"
     ]
    }
   ],
   "source": [
    "fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2],\n",
    "             labels[n_train:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
