{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用重复元素的网络(VGG)\n",
    "$AlexNet$在$LeNet$的基础上增加了$3$个卷积层。但$AlexNet$作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然$AlexNet$指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。我们将在本章的后续几节里介绍几种不同的深度网络设计思路。\n",
    "\n",
    "本节介绍$VGG$，$VGG$提出了可以通过**重复使用简单的基础块**来构建深度模型的思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG块\n",
    "$VGG$块的组成规律是：连续使用数个相同的填充为$1$、窗口形状为$3\\times 3$的卷积层后接上一个步幅为$2$、窗口形状为$2\\times 2$的最大池化层。**卷积层保持输入的高和宽不变，而池化层则对其减半**。\n",
    "\n",
    "对于给定的感受野（与输出有关的输入图片的局部大小），采用**堆积的小卷积核优于采用大的卷积核**，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在$VGG$中，使用了$3$个$3\\times 3$卷积核来代替$7\\times 7$卷积核，使用了$2$个$3\\times 3$卷积核来代替$5\\times 5$卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    blk = []\n",
    "    for i in range(num_convs):\n",
    "        if i == 0:\n",
    "            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "    blk.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG网络\n",
    "与$AlexNet$和$LeNet$一样，$VGG$网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个$vgg\\_block$，其超参数由变量$conv\\_arch$定义。该变量指定了每个$VGG$块里卷积层个数和输入输出通道数。全连接模块则跟$AlexNet$中的一样。\n",
    "\n",
    "现在我们构造一个$VGG$网络。它有$5$个卷积块，前$2$块使用单卷积层，而后$3$块使用双卷积层。第一块的输入输出通道分别是$1$（因为下面要使用的$Fashion-MNIST$数据的通道数为$1$）和$64$，之后每次对输出通道数翻倍，直到变为$512$。因为这个网络使用了$8$个卷积层和$3$个全连接层，所以经常被称为$VGG-11$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n",
    "# 经过5个vgg_block，宽高会减半五次，变成224/32=7\n",
    "fc_features = 512*7*7\n",
    "fc_hidden_units = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(conv_arch, fc_features, fc_hidden_units=4096):\n",
    "    net = nn.Sequential()\n",
    "    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n",
    "        net.add_module(\"vgg_block_\"+str(i+1), vgg_block(num_convs, in_channels,out_channels))\n",
    "    # 全连接层\n",
    "    net.add_module(\"fc\", nn.Sequential(\n",
    "        d2l.FlattenLayer(),\n",
    "        nn.Linear(fc_features, fc_hidden_units),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(fc_hidden_units, fc_hidden_units),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(fc_hidden_units, 10)\n",
    "    ))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面构造一个高和宽均为$224$的单通道数据样本来观察每一层的输出形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_block_1 output shape torch.Size([1, 64, 112, 112])\n",
      "vgg_block_2 output shape torch.Size([1, 128, 56, 56])\n",
      "vgg_block_3 output shape torch.Size([1, 256, 28, 28])\n",
      "vgg_block_4 output shape torch.Size([1, 512, 14, 14])\n",
      "vgg_block_5 output shape torch.Size([1, 512, 7, 7])\n",
      "fc output shape torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = vgg(conv_arch, fc_features, fc_hidden_units)\n",
    "X = torch.rand(1, 1, 224, 224)\n",
    "# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)\n",
    "for name, blk in net.named_children():\n",
    "    X = blk(X)\n",
    "    print(name, 'output shape', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成$7$后传入全连接层。与此同时，输出通道数每次翻倍，直到变成$512$。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。$VGG$这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据和训练模型\n",
    "因为$VGG-11$计算上比$AlexNet$更加复杂，出于测试的目的我们构造一个通道数更小，或者说更窄的网络在$Fashion-MNIST$数据集上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (vgg_block_1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_block_5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): FlattenLayer()\n",
      "    (1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ratio = 8\n",
    "small_conv_arch = [\n",
    "    (1, 1, 64//ratio),\n",
    "    (1, 64//ratio, 128//ratio),\n",
    "    (2, 128//ratio, 256//ratio),\n",
    "    (2, 256//ratio, 512//ratio),\n",
    "    (2, 512//ratio, 512//ratio)\n",
    "]\n",
    "net = vgg(small_conv_arch, fc_features//ratio, fc_hidden_units//ratio)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cuda:7\n",
      "epoch 1, loss 0.5153, train acc 0.813, test acc 0.865, time 37.0 sec\n",
      "epoch 2, loss 0.3368, train acc 0.878, test acc 0.889, time 37.1 sec\n",
      "epoch 3, loss 0.2947, train acc 0.894, test acc 0.893, time 36.8 sec\n",
      "epoch 4, loss 0.2655, train acc 0.904, test acc 0.902, time 37.2 sec\n",
      "epoch 5, loss 0.2432, train acc 0.912, test acc 0.907, time 36.4 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224, root='~/wms/Jupyter/Datasets/FashionMNIST')\n",
    "\n",
    "lr, epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyc",
   "language": "python",
   "name": "fyc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
