{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度卷积神经网络（AlexNet）\n",
    "计算机视觉流程中真正重要的是数据和特征。也就是说，使用较**干净的数据集和较有效的特征**甚至比机器学习模型的选择对图像分类结果的影响更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习表示特征\n",
    "既然特征如此重要，它该如何表示呢？\n",
    "\n",
    "我们已经提到，在相当长的时间里，特征都是基于各式各样手工设计的函数从数据中提取的。事实上，不少研究者通过提出新的特征提取函数不断改进图像分类结果。这一度为计算机视觉的发展做出了重要贡献。\n",
    "\n",
    "然而，另一些研究者则持异议。他们认为特征本身也应该由学习得来。他们还相信，为了表征足够复杂的输入，特征本身应该分级表示。持这一想法的研究者相信，多层神经网络可能可以学得数据的多级表征，并逐级表示越来越抽象的概念或模式。以图像分类为例，并回忆5.1节（二维卷积层）中物体边缘检测的例子。在多层神经网络中，图像的第一级的表示可以是在特定的位置和⻆度是否出现边缘；而第二级的表示说不定能够将这些边缘组合出有趣的模式，如花纹；在第三级的表示中，也许上一级的花纹能进一步汇合成对应物体特定部位的模式。这样逐级表示下去，最终，模型能够较容易根据最后一级的表示完成分类任务。需要强调的是，输入的逐级表示由多层模型中的参数决定，而这些参数都是学出来的。\n",
    "\n",
    "尽管一直有一群执着的研究者不断钻研，试图学习视觉数据的逐级表征，然而很长一段时间里这些野心都未能实现。这其中有诸多因素值得我们一一分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**缺失要素一：数据**\n",
    "\n",
    "包含许多特征的深度模型需要**大量的有标签的数据**才能表现得比其他经典方法更好。限于早期计算机有限的存储和$90$年代有限的研究预算，大部分研究只基于小的公开数据集。例如，不少研究论文基于加州大学欧文分校$（UCI）$提供的若干个公开数据集，其中许多数据集只有几百至几千张图像。这一状况在$2010$年前后兴起的大数据浪潮中得到改善。特别是，$2009$年诞生的ImageNet数据集包含了$1,000$大类物体，每类有多达数千张不同的图像。这一规模是当时其他公开数据集无法与之相提并论的。$ImageNet$数据集同时推动计算机视觉和机器学习研究进入新的阶段，使此前的传统方法不再有优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**缺失要素二：硬件**\n",
    "\n",
    "深度学习对计算资源要求很高。早期的硬件计算能力有限，这使训练较复杂的神经网络变得很困难。然而，**通用GPU的到来改变了这一格局**。很久以来，$GPU$都是为图像处理和计算机游戏设计的，尤其是针对大吞吐量的矩阵和向量乘法从而服务于基本的图形变换。值得庆幸的是，这其中的数学表达与深度网络中的卷积层的表达类似。通用$GPU$这个概念在$2001$年开始兴起，涌现出诸如$OpenCL$和$CUDA$之类的编程框架。这使得$GPU$也在$2010$年前后开始被机器学习社区使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "AlexNet与LeNet的设计理念非常相似，但也有显著的区别。\n",
    "\n",
    "第一，与相对较小的$LeNet$相比，$AlexNet$包含 **8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层**。下面我们来详细描述这些层的设计。\n",
    "\n",
    "$AlexNet$第一层中的卷积窗口形状是$11\\times 11$。因为$ImageNet$中绝大多数图像的高和宽均比$MNIST$图像的高和宽大$10$倍以上，$ImageNet$图像的物体占用更多的像素，所以需要更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到$5\\times 5$，之后全采用$3\\times 3$。此外，第一、第二和第五个卷积层之后都使用了窗口形状为$3\\times 3$、步幅为$2$的最大池化层。而且，$AlexNet$使用的卷积通道数也大于$LeNet$中的卷积通道数数十倍。\n",
    "\n",
    "紧接着最后一个卷积层的是两个输出个数为$4096$的全连接层。这两个巨大的全连接层带来将近$1$ $GB$的模型参数。由于早期显存的限制，最早的$AlexNet$使用双数据流的设计使一个$GPU$只需要处理一半模型。幸运的是，显存在过去几年得到了长足的发展，因此通常我们不再需要这样的特别设计了。\n",
    "\n",
    "第二，$AlexNet$将$sigmoid$激活函数改成了更加简单的$ReLU$激活函数。一方面，$ReLU$激活函数的**计算更简单**，例如它并没有$sigmoid$激活函数中的求幂运算。另一方面，$ReLU$激活函数在不同的参数初始化方法下使模型更容易训练。这是由于当$sigmoid$激活函数输出极接近$0$或$1$时，这些区域的梯度几乎为$0$，从而造成反向传播无法继续更新部分模型参数；而$ReLU$激活函数在**正区间的梯度恒为1**。因此，若模型参数初始化不当，$sigmoid$函数可能在正区间得到几乎为$0$的梯度，从而令模型无法得到有效训练。\n",
    "\n",
    "第三，$AlexNet$通过**丢弃法**（参见3.13节）来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。\n",
    "\n",
    "第四，$AlexNet$引入了大量的**图像增广，如翻转、裁剪和颜色变化**，从而进一步扩大数据集来缓解过拟合。我们将在后面的9.1节（图像增广）详细介绍这种方法。\n",
    "\n",
    "下面我们实现稍微简化过的$AlexNet$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, 11, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*5*5, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        return self.fc(feature.view(img.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据\n",
    "虽然论文中$AlexNet$使用$ImageNet$数据集，但因为$ImageNet$数据集训练时间较长，我们仍用前面的$Fashion-MNIST$数据集来演示$AlexNet$。读取数据的时候我们额外做了一步将图像高和宽扩大到$AlexNet$使用的图像高和宽$224$。这个可以通过$torchvision.transforms.Resize$实例来实现。也就是说，我们在$ToTensor$实例前使用$Resize$实例，然后使用$Compose$实例来将这两个变换串联以方便调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None, root='~/DL/Datasets/FashionMNIST'):\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size = resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    return train_iter, test_iter\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "这时候我们可以开始训练$AlexNet$了。相对于$LeNet$，由于图片尺寸变大了而且模型变大了，所以需要更大的显存，也需要更长的训练时间了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cpu\n",
      "step 1, train_acc: 0.1250\n",
      "step 2, train_acc: 0.1133\n",
      "step 3, train_acc: 0.1016\n",
      "step 4, train_acc: 0.1074\n",
      "step 5, train_acc: 0.1031\n",
      "step 6, train_acc: 0.1029\n",
      "step 7, train_acc: 0.1105\n",
      "step 8, train_acc: 0.1104\n",
      "step 9, train_acc: 0.1102\n",
      "step 10, train_acc: 0.1164\n",
      "step 11, train_acc: 0.1229\n",
      "step 12, train_acc: 0.1237\n",
      "step 13, train_acc: 0.1340\n",
      "step 14, train_acc: 0.1501\n",
      "step 15, train_acc: 0.1599\n",
      "step 16, train_acc: 0.1709\n",
      "step 17, train_acc: 0.1788\n",
      "step 18, train_acc: 0.1931\n",
      "step 19, train_acc: 0.2039\n",
      "step 20, train_acc: 0.2113\n",
      "step 21, train_acc: 0.2217\n",
      "step 22, train_acc: 0.2330\n",
      "step 23, train_acc: 0.2415\n",
      "step 24, train_acc: 0.2477\n",
      "step 25, train_acc: 0.2544\n",
      "step 26, train_acc: 0.2629\n",
      "step 27, train_acc: 0.2691\n",
      "step 28, train_acc: 0.2771\n",
      "step 29, train_acc: 0.2815\n",
      "step 30, train_acc: 0.2862\n",
      "step 31, train_acc: 0.2883\n",
      "step 32, train_acc: 0.2942\n",
      "step 33, train_acc: 0.3023\n",
      "step 34, train_acc: 0.3095\n",
      "step 35, train_acc: 0.3170\n",
      "step 36, train_acc: 0.3242\n",
      "step 37, train_acc: 0.3313\n",
      "step 38, train_acc: 0.3353\n",
      "step 39, train_acc: 0.3425\n",
      "step 40, train_acc: 0.3482\n",
      "step 41, train_acc: 0.3527\n",
      "step 42, train_acc: 0.3575\n",
      "step 43, train_acc: 0.3625\n",
      "step 44, train_acc: 0.3667\n",
      "step 45, train_acc: 0.3719\n",
      "step 46, train_acc: 0.3775\n",
      "step 47, train_acc: 0.3816\n",
      "step 48, train_acc: 0.3872\n",
      "step 49, train_acc: 0.3925\n",
      "step 50, train_acc: 0.3967\n",
      "step 51, train_acc: 0.4013\n",
      "step 52, train_acc: 0.4064\n",
      "step 53, train_acc: 0.4110\n",
      "step 54, train_acc: 0.4141\n",
      "step 55, train_acc: 0.4196\n",
      "step 56, train_acc: 0.4237\n",
      "step 57, train_acc: 0.4271\n",
      "step 58, train_acc: 0.4309\n",
      "step 59, train_acc: 0.4335\n",
      "step 60, train_acc: 0.4372\n",
      "step 61, train_acc: 0.4410\n",
      "step 62, train_acc: 0.4442\n",
      "step 63, train_acc: 0.4475\n",
      "step 64, train_acc: 0.4512\n",
      "step 65, train_acc: 0.4556\n",
      "step 66, train_acc: 0.4583\n",
      "step 67, train_acc: 0.4619\n",
      "step 68, train_acc: 0.4653\n",
      "step 69, train_acc: 0.4678\n",
      "step 70, train_acc: 0.4713\n",
      "step 71, train_acc: 0.4748\n",
      "step 72, train_acc: 0.4779\n",
      "step 73, train_acc: 0.4810\n",
      "step 74, train_acc: 0.4838\n",
      "step 75, train_acc: 0.4866\n",
      "step 76, train_acc: 0.4896\n",
      "step 77, train_acc: 0.4931\n",
      "step 78, train_acc: 0.4967\n",
      "step 79, train_acc: 0.4997\n",
      "step 80, train_acc: 0.5014\n",
      "step 81, train_acc: 0.5041\n",
      "step 82, train_acc: 0.5063\n",
      "step 83, train_acc: 0.5092\n",
      "step 84, train_acc: 0.5120\n",
      "step 85, train_acc: 0.5142\n",
      "step 86, train_acc: 0.5169\n",
      "step 87, train_acc: 0.5196\n",
      "step 88, train_acc: 0.5218\n",
      "step 89, train_acc: 0.5246\n",
      "step 90, train_acc: 0.5275\n",
      "step 91, train_acc: 0.5302\n",
      "step 92, train_acc: 0.5317\n",
      "step 93, train_acc: 0.5347\n",
      "step 94, train_acc: 0.5375\n",
      "step 95, train_acc: 0.5393\n",
      "step 96, train_acc: 0.5408\n",
      "step 97, train_acc: 0.5420\n",
      "step 98, train_acc: 0.5436\n",
      "step 99, train_acc: 0.5464\n",
      "step 100, train_acc: 0.5476\n",
      "step 101, train_acc: 0.5497\n",
      "step 102, train_acc: 0.5515\n",
      "step 103, train_acc: 0.5537\n",
      "step 104, train_acc: 0.5552\n",
      "step 105, train_acc: 0.5574\n",
      "step 106, train_acc: 0.5596\n",
      "step 107, train_acc: 0.5612\n",
      "step 108, train_acc: 0.5632\n",
      "step 109, train_acc: 0.5653\n",
      "step 110, train_acc: 0.5669\n",
      "step 111, train_acc: 0.5682\n",
      "step 112, train_acc: 0.5696\n",
      "step 113, train_acc: 0.5711\n",
      "step 114, train_acc: 0.5731\n",
      "step 115, train_acc: 0.5749\n",
      "step 116, train_acc: 0.5762\n",
      "step 117, train_acc: 0.5778\n",
      "step 118, train_acc: 0.5791\n",
      "step 119, train_acc: 0.5808\n",
      "step 120, train_acc: 0.5823\n",
      "step 121, train_acc: 0.5840\n",
      "step 122, train_acc: 0.5859\n",
      "step 123, train_acc: 0.5879\n",
      "step 124, train_acc: 0.5898\n",
      "step 125, train_acc: 0.5913\n",
      "step 126, train_acc: 0.5926\n",
      "step 127, train_acc: 0.5942\n",
      "step 128, train_acc: 0.5955\n",
      "step 129, train_acc: 0.5968\n",
      "step 130, train_acc: 0.5978\n",
      "step 131, train_acc: 0.5994\n",
      "step 132, train_acc: 0.6009\n",
      "step 133, train_acc: 0.6021\n",
      "step 134, train_acc: 0.6038\n",
      "step 135, train_acc: 0.6049\n",
      "step 136, train_acc: 0.6064\n",
      "step 137, train_acc: 0.6074\n",
      "step 138, train_acc: 0.6089\n",
      "step 139, train_acc: 0.6100\n",
      "step 140, train_acc: 0.6113\n",
      "step 141, train_acc: 0.6129\n",
      "step 142, train_acc: 0.6135\n",
      "step 143, train_acc: 0.6151\n",
      "step 144, train_acc: 0.6160\n",
      "step 145, train_acc: 0.6171\n",
      "step 146, train_acc: 0.6178\n",
      "step 147, train_acc: 0.6188\n",
      "step 148, train_acc: 0.6202\n",
      "step 149, train_acc: 0.6209\n",
      "step 150, train_acc: 0.6221\n",
      "step 151, train_acc: 0.6236\n",
      "step 152, train_acc: 0.6246\n",
      "step 153, train_acc: 0.6260\n",
      "step 154, train_acc: 0.6273\n",
      "step 155, train_acc: 0.6283\n",
      "step 156, train_acc: 0.6295\n",
      "step 157, train_acc: 0.6308\n",
      "step 158, train_acc: 0.6315\n",
      "step 159, train_acc: 0.6325\n",
      "step 160, train_acc: 0.6333\n",
      "step 161, train_acc: 0.6343\n",
      "step 162, train_acc: 0.6353\n",
      "step 163, train_acc: 0.6361\n",
      "step 164, train_acc: 0.6371\n",
      "step 165, train_acc: 0.6381\n",
      "step 166, train_acc: 0.6396\n",
      "step 167, train_acc: 0.6406\n",
      "step 168, train_acc: 0.6412\n",
      "step 169, train_acc: 0.6422\n",
      "step 170, train_acc: 0.6434\n",
      "step 171, train_acc: 0.6439\n",
      "step 172, train_acc: 0.6446\n",
      "step 173, train_acc: 0.6452\n",
      "step 174, train_acc: 0.6462\n",
      "step 175, train_acc: 0.6473\n",
      "step 176, train_acc: 0.6475\n",
      "step 177, train_acc: 0.6482\n",
      "step 178, train_acc: 0.6487\n",
      "step 179, train_acc: 0.6497\n",
      "step 180, train_acc: 0.6503\n",
      "step 181, train_acc: 0.6511\n",
      "step 182, train_acc: 0.6520\n",
      "step 183, train_acc: 0.6526\n",
      "step 184, train_acc: 0.6534\n",
      "step 185, train_acc: 0.6543\n",
      "step 186, train_acc: 0.6550\n",
      "step 187, train_acc: 0.6561\n",
      "step 188, train_acc: 0.6571\n",
      "step 189, train_acc: 0.6579\n",
      "step 190, train_acc: 0.6589\n",
      "step 191, train_acc: 0.6596\n",
      "step 192, train_acc: 0.6605\n",
      "step 193, train_acc: 0.6615\n",
      "step 194, train_acc: 0.6623\n",
      "step 195, train_acc: 0.6631\n",
      "step 196, train_acc: 0.6637\n",
      "step 197, train_acc: 0.6643\n",
      "step 198, train_acc: 0.6649\n",
      "step 199, train_acc: 0.6659\n",
      "step 200, train_acc: 0.6668\n",
      "step 201, train_acc: 0.6674\n",
      "step 202, train_acc: 0.6682\n",
      "step 203, train_acc: 0.6688\n",
      "step 204, train_acc: 0.6694\n",
      "step 205, train_acc: 0.6702\n",
      "step 206, train_acc: 0.6710\n",
      "step 207, train_acc: 0.6717\n",
      "step 208, train_acc: 0.6726\n",
      "step 209, train_acc: 0.6731\n",
      "step 210, train_acc: 0.6738\n",
      "step 211, train_acc: 0.6744\n",
      "step 212, train_acc: 0.6751\n",
      "step 213, train_acc: 0.6759\n",
      "step 214, train_acc: 0.6763\n",
      "step 215, train_acc: 0.6767\n",
      "step 216, train_acc: 0.6774\n",
      "step 217, train_acc: 0.6781\n",
      "step 218, train_acc: 0.6786\n",
      "step 219, train_acc: 0.6789\n",
      "step 220, train_acc: 0.6796\n",
      "step 221, train_acc: 0.6801\n",
      "step 222, train_acc: 0.6807\n",
      "step 223, train_acc: 0.6811\n",
      "step 224, train_acc: 0.6816\n",
      "step 225, train_acc: 0.6823\n",
      "step 226, train_acc: 0.6829\n",
      "step 227, train_acc: 0.6835\n",
      "step 228, train_acc: 0.6842\n",
      "step 229, train_acc: 0.6847\n",
      "step 230, train_acc: 0.6853\n",
      "step 231, train_acc: 0.6856\n",
      "step 232, train_acc: 0.6860\n",
      "step 233, train_acc: 0.6866\n",
      "step 234, train_acc: 0.6871\n",
      "step 235, train_acc: 0.6876\n",
      "step 236, train_acc: 0.6883\n",
      "step 237, train_acc: 0.6889\n",
      "step 238, train_acc: 0.6896\n",
      "step 239, train_acc: 0.6900\n",
      "step 240, train_acc: 0.6905\n",
      "step 241, train_acc: 0.6907\n",
      "step 242, train_acc: 0.6915\n",
      "step 243, train_acc: 0.6918\n",
      "step 244, train_acc: 0.6920\n",
      "step 245, train_acc: 0.6927\n",
      "step 246, train_acc: 0.6930\n",
      "step 247, train_acc: 0.6935\n",
      "step 248, train_acc: 0.6941\n",
      "step 249, train_acc: 0.6948\n",
      "step 250, train_acc: 0.6954\n",
      "step 251, train_acc: 0.6957\n",
      "step 252, train_acc: 0.6964\n",
      "step 253, train_acc: 0.6968\n",
      "step 254, train_acc: 0.6973\n",
      "step 255, train_acc: 0.6980\n",
      "step 256, train_acc: 0.6985\n",
      "step 257, train_acc: 0.6988\n",
      "step 258, train_acc: 0.6989\n",
      "step 259, train_acc: 0.6994\n",
      "step 260, train_acc: 0.6997\n",
      "step 261, train_acc: 0.7003\n",
      "step 262, train_acc: 0.7006\n",
      "step 263, train_acc: 0.7012\n",
      "step 264, train_acc: 0.7017\n",
      "step 265, train_acc: 0.7024\n",
      "step 266, train_acc: 0.7030\n",
      "step 267, train_acc: 0.7035\n",
      "step 268, train_acc: 0.7039\n",
      "step 269, train_acc: 0.7042\n",
      "step 270, train_acc: 0.7049\n",
      "step 271, train_acc: 0.7054\n",
      "step 272, train_acc: 0.7058\n",
      "step 273, train_acc: 0.7061\n",
      "step 274, train_acc: 0.7063\n",
      "step 275, train_acc: 0.7068\n",
      "step 276, train_acc: 0.7072\n",
      "step 277, train_acc: 0.7074\n",
      "step 278, train_acc: 0.7079\n",
      "step 279, train_acc: 0.7083\n",
      "step 280, train_acc: 0.7089\n",
      "step 281, train_acc: 0.7092\n",
      "step 282, train_acc: 0.7096\n",
      "step 283, train_acc: 0.7101\n",
      "step 284, train_acc: 0.7106\n",
      "step 285, train_acc: 0.7110\n",
      "step 286, train_acc: 0.7113\n",
      "step 287, train_acc: 0.7118\n",
      "step 288, train_acc: 0.7123\n",
      "step 289, train_acc: 0.7127\n",
      "step 290, train_acc: 0.7131\n",
      "step 291, train_acc: 0.7135\n",
      "step 292, train_acc: 0.7139\n",
      "step 293, train_acc: 0.7145\n",
      "step 294, train_acc: 0.7150\n",
      "step 295, train_acc: 0.7155\n",
      "step 296, train_acc: 0.7159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 297, train_acc: 0.7161\n",
      "step 298, train_acc: 0.7166\n",
      "step 299, train_acc: 0.7173\n",
      "step 300, train_acc: 0.7176\n",
      "step 301, train_acc: 0.7179\n",
      "step 302, train_acc: 0.7183\n",
      "step 303, train_acc: 0.7185\n",
      "step 304, train_acc: 0.7189\n",
      "step 305, train_acc: 0.7193\n",
      "step 306, train_acc: 0.7196\n",
      "step 307, train_acc: 0.7200\n",
      "step 308, train_acc: 0.7203\n",
      "step 309, train_acc: 0.7206\n",
      "step 310, train_acc: 0.7210\n",
      "step 311, train_acc: 0.7215\n",
      "step 312, train_acc: 0.7217\n",
      "step 313, train_acc: 0.7222\n",
      "step 314, train_acc: 0.7225\n",
      "step 315, train_acc: 0.7229\n",
      "step 316, train_acc: 0.7234\n",
      "step 317, train_acc: 0.7239\n",
      "step 318, train_acc: 0.7242\n",
      "step 319, train_acc: 0.7246\n",
      "step 320, train_acc: 0.7249\n",
      "step 321, train_acc: 0.7253\n",
      "step 322, train_acc: 0.7256\n",
      "step 323, train_acc: 0.7260\n",
      "step 324, train_acc: 0.7264\n",
      "step 325, train_acc: 0.7266\n",
      "step 326, train_acc: 0.7269\n",
      "step 327, train_acc: 0.7273\n",
      "step 328, train_acc: 0.7277\n",
      "step 329, train_acc: 0.7279\n",
      "step 330, train_acc: 0.7282\n",
      "step 331, train_acc: 0.7284\n",
      "step 332, train_acc: 0.7288\n",
      "step 333, train_acc: 0.7292\n",
      "step 334, train_acc: 0.7296\n",
      "step 335, train_acc: 0.7298\n",
      "step 336, train_acc: 0.7302\n",
      "step 337, train_acc: 0.7304\n",
      "step 338, train_acc: 0.7306\n",
      "step 339, train_acc: 0.7310\n",
      "step 340, train_acc: 0.7313\n",
      "step 341, train_acc: 0.7316\n",
      "step 342, train_acc: 0.7318\n",
      "step 343, train_acc: 0.7322\n",
      "step 344, train_acc: 0.7324\n",
      "step 345, train_acc: 0.7327\n",
      "step 346, train_acc: 0.7332\n",
      "step 347, train_acc: 0.7336\n",
      "step 348, train_acc: 0.7339\n",
      "step 349, train_acc: 0.7344\n",
      "step 350, train_acc: 0.7346\n",
      "step 351, train_acc: 0.7349\n",
      "step 352, train_acc: 0.7351\n",
      "step 353, train_acc: 0.7353\n",
      "step 354, train_acc: 0.7357\n",
      "step 355, train_acc: 0.7361\n",
      "step 356, train_acc: 0.7364\n",
      "step 357, train_acc: 0.7367\n",
      "step 358, train_acc: 0.7370\n",
      "step 359, train_acc: 0.7375\n",
      "step 360, train_acc: 0.7379\n",
      "step 361, train_acc: 0.7383\n",
      "step 362, train_acc: 0.7388\n",
      "step 363, train_acc: 0.7390\n",
      "step 364, train_acc: 0.7393\n",
      "step 365, train_acc: 0.7395\n",
      "step 366, train_acc: 0.7399\n",
      "step 367, train_acc: 0.7404\n",
      "step 368, train_acc: 0.7406\n",
      "step 369, train_acc: 0.7410\n",
      "step 370, train_acc: 0.7412\n",
      "step 371, train_acc: 0.7415\n",
      "step 372, train_acc: 0.7417\n",
      "step 373, train_acc: 0.7417\n",
      "step 374, train_acc: 0.7420\n",
      "step 375, train_acc: 0.7424\n",
      "step 376, train_acc: 0.7427\n",
      "step 377, train_acc: 0.7430\n",
      "step 378, train_acc: 0.7431\n",
      "step 379, train_acc: 0.7433\n",
      "step 380, train_acc: 0.7436\n",
      "step 381, train_acc: 0.7439\n",
      "step 382, train_acc: 0.7441\n",
      "step 383, train_acc: 0.7443\n",
      "step 384, train_acc: 0.7443\n",
      "step 385, train_acc: 0.7446\n",
      "step 386, train_acc: 0.7449\n",
      "step 387, train_acc: 0.7453\n",
      "step 388, train_acc: 0.7456\n",
      "step 389, train_acc: 0.7459\n",
      "step 390, train_acc: 0.7461\n",
      "step 391, train_acc: 0.7464\n",
      "step 392, train_acc: 0.7464\n",
      "step 393, train_acc: 0.7467\n",
      "step 394, train_acc: 0.7470\n",
      "step 395, train_acc: 0.7473\n",
      "step 396, train_acc: 0.7476\n",
      "step 397, train_acc: 0.7480\n",
      "step 398, train_acc: 0.7481\n",
      "step 399, train_acc: 0.7484\n",
      "step 400, train_acc: 0.7487\n",
      "step 401, train_acc: 0.7491\n",
      "step 402, train_acc: 0.7494\n",
      "step 403, train_acc: 0.7497\n",
      "step 404, train_acc: 0.7499\n",
      "step 405, train_acc: 0.7502\n",
      "step 406, train_acc: 0.7506\n",
      "step 407, train_acc: 0.7508\n",
      "step 408, train_acc: 0.7509\n",
      "step 409, train_acc: 0.7510\n",
      "step 410, train_acc: 0.7513\n",
      "step 411, train_acc: 0.7516\n",
      "step 412, train_acc: 0.7517\n",
      "step 413, train_acc: 0.7519\n",
      "step 414, train_acc: 0.7522\n",
      "step 415, train_acc: 0.7524\n",
      "step 416, train_acc: 0.7526\n",
      "step 417, train_acc: 0.7529\n",
      "step 418, train_acc: 0.7530\n",
      "step 419, train_acc: 0.7532\n",
      "step 420, train_acc: 0.7534\n",
      "step 421, train_acc: 0.7537\n",
      "step 422, train_acc: 0.7539\n",
      "step 423, train_acc: 0.7542\n",
      "step 424, train_acc: 0.7545\n",
      "step 425, train_acc: 0.7548\n",
      "step 426, train_acc: 0.7550\n",
      "step 427, train_acc: 0.7553\n",
      "step 428, train_acc: 0.7555\n",
      "step 429, train_acc: 0.7558\n",
      "step 430, train_acc: 0.7560\n",
      "step 431, train_acc: 0.7562\n",
      "step 432, train_acc: 0.7564\n",
      "step 433, train_acc: 0.7566\n",
      "step 434, train_acc: 0.7568\n",
      "step 435, train_acc: 0.7570\n",
      "step 436, train_acc: 0.7572\n",
      "step 437, train_acc: 0.7575\n",
      "step 438, train_acc: 0.7578\n",
      "step 439, train_acc: 0.7580\n",
      "step 440, train_acc: 0.7581\n",
      "step 441, train_acc: 0.7584\n",
      "step 442, train_acc: 0.7586\n",
      "step 443, train_acc: 0.7590\n",
      "step 444, train_acc: 0.7592\n",
      "step 445, train_acc: 0.7594\n",
      "step 446, train_acc: 0.7595\n",
      "step 447, train_acc: 0.7596\n",
      "step 448, train_acc: 0.7598\n",
      "step 449, train_acc: 0.7600\n",
      "step 450, train_acc: 0.7602\n",
      "step 451, train_acc: 0.7604\n",
      "step 452, train_acc: 0.7605\n",
      "step 453, train_acc: 0.7607\n",
      "step 454, train_acc: 0.7609\n",
      "step 455, train_acc: 0.7611\n",
      "step 456, train_acc: 0.7613\n",
      "step 457, train_acc: 0.7615\n",
      "step 458, train_acc: 0.7618\n",
      "step 459, train_acc: 0.7620\n",
      "step 460, train_acc: 0.7622\n",
      "step 461, train_acc: 0.7624\n",
      "step 462, train_acc: 0.7627\n",
      "step 463, train_acc: 0.7628\n",
      "step 464, train_acc: 0.7630\n",
      "step 465, train_acc: 0.7633\n",
      "step 466, train_acc: 0.7636\n",
      "step 467, train_acc: 0.7637\n",
      "step 468, train_acc: 0.7639\n",
      "step 469, train_acc: 0.7641\n",
      "epoch 1, loss 0.6217, train acc 0.764, test acc 0.856, time 3646.4 sec\n",
      "step 1, train_acc: 0.8672\n",
      "step 2, train_acc: 0.8711\n",
      "step 3, train_acc: 0.8672\n",
      "step 4, train_acc: 0.8672\n",
      "step 5, train_acc: 0.8625\n",
      "step 6, train_acc: 0.8789\n",
      "step 7, train_acc: 0.8705\n",
      "step 8, train_acc: 0.8750\n",
      "step 9, train_acc: 0.8741\n",
      "step 10, train_acc: 0.8727\n",
      "step 11, train_acc: 0.8722\n",
      "step 12, train_acc: 0.8704\n",
      "step 13, train_acc: 0.8708\n",
      "step 14, train_acc: 0.8683\n",
      "step 15, train_acc: 0.8698\n",
      "step 16, train_acc: 0.8706\n",
      "step 17, train_acc: 0.8681\n",
      "step 18, train_acc: 0.8668\n",
      "step 19, train_acc: 0.8680\n",
      "step 20, train_acc: 0.8676\n",
      "step 21, train_acc: 0.8650\n",
      "step 22, train_acc: 0.8643\n",
      "step 23, train_acc: 0.8648\n",
      "step 24, train_acc: 0.8643\n",
      "step 25, train_acc: 0.8625\n",
      "step 26, train_acc: 0.8639\n",
      "step 27, train_acc: 0.8640\n",
      "step 28, train_acc: 0.8636\n",
      "step 29, train_acc: 0.8623\n",
      "step 30, train_acc: 0.8625\n",
      "step 31, train_acc: 0.8616\n",
      "step 32, train_acc: 0.8611\n",
      "step 33, train_acc: 0.8610\n",
      "step 34, train_acc: 0.8598\n",
      "step 35, train_acc: 0.8598\n",
      "step 36, train_acc: 0.8598\n",
      "step 37, train_acc: 0.8590\n",
      "step 38, train_acc: 0.8596\n",
      "step 39, train_acc: 0.8592\n",
      "step 40, train_acc: 0.8596\n",
      "step 41, train_acc: 0.8613\n",
      "step 42, train_acc: 0.8614\n",
      "step 43, train_acc: 0.8617\n",
      "step 44, train_acc: 0.8617\n",
      "step 45, train_acc: 0.8618\n",
      "step 46, train_acc: 0.8626\n",
      "step 47, train_acc: 0.8624\n",
      "step 48, train_acc: 0.8620\n",
      "step 49, train_acc: 0.8626\n",
      "step 50, train_acc: 0.8625\n",
      "step 51, train_acc: 0.8609\n",
      "step 52, train_acc: 0.8621\n",
      "step 53, train_acc: 0.8622\n",
      "step 54, train_acc: 0.8621\n",
      "step 55, train_acc: 0.8622\n",
      "step 56, train_acc: 0.8617\n",
      "step 57, train_acc: 0.8623\n",
      "step 58, train_acc: 0.8625\n",
      "step 59, train_acc: 0.8627\n",
      "step 60, train_acc: 0.8632\n",
      "step 61, train_acc: 0.8626\n",
      "step 62, train_acc: 0.8621\n",
      "step 63, train_acc: 0.8627\n",
      "step 64, train_acc: 0.8623\n",
      "step 65, train_acc: 0.8620\n",
      "step 66, train_acc: 0.8622\n",
      "step 67, train_acc: 0.8624\n",
      "step 68, train_acc: 0.8619\n",
      "step 69, train_acc: 0.8625\n",
      "step 70, train_acc: 0.8622\n",
      "step 71, train_acc: 0.8615\n",
      "step 72, train_acc: 0.8612\n",
      "step 73, train_acc: 0.8614\n",
      "step 74, train_acc: 0.8615\n",
      "step 75, train_acc: 0.8622\n",
      "step 76, train_acc: 0.8622\n",
      "step 77, train_acc: 0.8624\n",
      "step 78, train_acc: 0.8628\n",
      "step 79, train_acc: 0.8627\n",
      "step 80, train_acc: 0.8626\n",
      "step 81, train_acc: 0.8628\n",
      "step 82, train_acc: 0.8632\n",
      "step 83, train_acc: 0.8633\n",
      "step 84, train_acc: 0.8634\n",
      "step 85, train_acc: 0.8631\n",
      "step 86, train_acc: 0.8630\n",
      "step 87, train_acc: 0.8631\n",
      "step 88, train_acc: 0.8632\n",
      "step 89, train_acc: 0.8631\n",
      "step 90, train_acc: 0.8632\n",
      "step 91, train_acc: 0.8631\n",
      "step 92, train_acc: 0.8631\n",
      "step 93, train_acc: 0.8625\n",
      "step 94, train_acc: 0.8627\n",
      "step 95, train_acc: 0.8632\n",
      "step 96, train_acc: 0.8634\n",
      "step 97, train_acc: 0.8632\n",
      "step 98, train_acc: 0.8632\n",
      "step 99, train_acc: 0.8630\n",
      "step 100, train_acc: 0.8628\n",
      "step 101, train_acc: 0.8632\n",
      "step 102, train_acc: 0.8632\n",
      "step 103, train_acc: 0.8636\n",
      "step 104, train_acc: 0.8640\n",
      "step 105, train_acc: 0.8641\n",
      "step 106, train_acc: 0.8644\n",
      "step 107, train_acc: 0.8646\n",
      "step 108, train_acc: 0.8644\n",
      "step 109, train_acc: 0.8640\n",
      "step 110, train_acc: 0.8641\n",
      "step 111, train_acc: 0.8639\n",
      "step 112, train_acc: 0.8642\n",
      "step 113, train_acc: 0.8643\n",
      "step 114, train_acc: 0.8645\n",
      "step 115, train_acc: 0.8644\n",
      "step 116, train_acc: 0.8642\n",
      "step 117, train_acc: 0.8647\n",
      "step 118, train_acc: 0.8649\n",
      "step 119, train_acc: 0.8650\n",
      "step 120, train_acc: 0.8654\n",
      "step 121, train_acc: 0.8653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 122, train_acc: 0.8651\n",
      "step 123, train_acc: 0.8651\n",
      "step 124, train_acc: 0.8650\n",
      "step 125, train_acc: 0.8652\n",
      "step 126, train_acc: 0.8650\n",
      "step 127, train_acc: 0.8651\n",
      "step 128, train_acc: 0.8651\n",
      "step 129, train_acc: 0.8653\n",
      "step 130, train_acc: 0.8651\n",
      "step 131, train_acc: 0.8652\n",
      "step 132, train_acc: 0.8653\n",
      "step 133, train_acc: 0.8654\n",
      "step 134, train_acc: 0.8653\n",
      "step 135, train_acc: 0.8652\n",
      "step 136, train_acc: 0.8652\n",
      "step 137, train_acc: 0.8651\n",
      "step 138, train_acc: 0.8654\n",
      "step 139, train_acc: 0.8653\n",
      "step 140, train_acc: 0.8655\n",
      "step 141, train_acc: 0.8657\n",
      "step 142, train_acc: 0.8656\n",
      "step 143, train_acc: 0.8655\n",
      "step 144, train_acc: 0.8659\n",
      "step 145, train_acc: 0.8665\n",
      "step 146, train_acc: 0.8667\n",
      "step 147, train_acc: 0.8668\n",
      "step 148, train_acc: 0.8663\n",
      "step 149, train_acc: 0.8662\n",
      "step 150, train_acc: 0.8662\n",
      "step 151, train_acc: 0.8664\n",
      "step 152, train_acc: 0.8663\n",
      "step 153, train_acc: 0.8665\n",
      "step 154, train_acc: 0.8666\n",
      "step 155, train_acc: 0.8666\n",
      "step 156, train_acc: 0.8664\n",
      "step 157, train_acc: 0.8662\n",
      "step 158, train_acc: 0.8661\n",
      "step 159, train_acc: 0.8661\n",
      "step 160, train_acc: 0.8664\n",
      "step 161, train_acc: 0.8666\n",
      "step 162, train_acc: 0.8666\n",
      "step 163, train_acc: 0.8669\n",
      "step 164, train_acc: 0.8671\n",
      "step 165, train_acc: 0.8671\n",
      "step 166, train_acc: 0.8674\n",
      "step 167, train_acc: 0.8673\n",
      "step 168, train_acc: 0.8673\n",
      "step 169, train_acc: 0.8675\n",
      "step 170, train_acc: 0.8676\n",
      "step 171, train_acc: 0.8678\n",
      "step 172, train_acc: 0.8677\n",
      "step 173, train_acc: 0.8675\n",
      "step 174, train_acc: 0.8675\n",
      "step 175, train_acc: 0.8673\n",
      "step 176, train_acc: 0.8674\n",
      "step 177, train_acc: 0.8674\n",
      "step 178, train_acc: 0.8669\n",
      "step 179, train_acc: 0.8671\n",
      "step 180, train_acc: 0.8669\n",
      "step 181, train_acc: 0.8669\n",
      "step 182, train_acc: 0.8671\n",
      "step 183, train_acc: 0.8673\n",
      "step 184, train_acc: 0.8672\n",
      "step 185, train_acc: 0.8673\n",
      "step 186, train_acc: 0.8674\n",
      "step 187, train_acc: 0.8673\n",
      "step 188, train_acc: 0.8674\n",
      "step 189, train_acc: 0.8674\n",
      "step 190, train_acc: 0.8674\n",
      "step 191, train_acc: 0.8675\n",
      "step 192, train_acc: 0.8678\n",
      "step 193, train_acc: 0.8679\n",
      "step 194, train_acc: 0.8674\n",
      "step 195, train_acc: 0.8673\n",
      "step 196, train_acc: 0.8671\n",
      "step 197, train_acc: 0.8669\n",
      "step 198, train_acc: 0.8668\n",
      "step 199, train_acc: 0.8666\n",
      "step 200, train_acc: 0.8664\n",
      "step 201, train_acc: 0.8665\n",
      "step 202, train_acc: 0.8667\n",
      "step 203, train_acc: 0.8667\n",
      "step 204, train_acc: 0.8668\n",
      "step 205, train_acc: 0.8670\n",
      "step 206, train_acc: 0.8671\n",
      "step 207, train_acc: 0.8670\n",
      "step 208, train_acc: 0.8669\n",
      "step 209, train_acc: 0.8669\n",
      "step 210, train_acc: 0.8670\n",
      "step 211, train_acc: 0.8672\n",
      "step 212, train_acc: 0.8672\n",
      "step 213, train_acc: 0.8673\n",
      "step 214, train_acc: 0.8673\n",
      "step 215, train_acc: 0.8673\n",
      "step 216, train_acc: 0.8675\n",
      "step 217, train_acc: 0.8674\n",
      "step 218, train_acc: 0.8677\n",
      "step 219, train_acc: 0.8679\n",
      "step 220, train_acc: 0.8679\n",
      "step 221, train_acc: 0.8679\n",
      "step 222, train_acc: 0.8681\n",
      "step 223, train_acc: 0.8682\n",
      "step 224, train_acc: 0.8683\n",
      "step 225, train_acc: 0.8684\n",
      "step 226, train_acc: 0.8685\n",
      "step 227, train_acc: 0.8685\n",
      "step 228, train_acc: 0.8687\n",
      "step 229, train_acc: 0.8689\n",
      "step 230, train_acc: 0.8690\n",
      "step 231, train_acc: 0.8690\n",
      "step 232, train_acc: 0.8692\n",
      "step 233, train_acc: 0.8692\n",
      "step 234, train_acc: 0.8691\n",
      "step 235, train_acc: 0.8691\n",
      "step 236, train_acc: 0.8690\n",
      "step 237, train_acc: 0.8689\n",
      "step 238, train_acc: 0.8689\n",
      "step 239, train_acc: 0.8691\n",
      "step 240, train_acc: 0.8690\n",
      "step 241, train_acc: 0.8691\n",
      "step 242, train_acc: 0.8692\n",
      "step 243, train_acc: 0.8693\n",
      "step 244, train_acc: 0.8695\n",
      "step 245, train_acc: 0.8695\n",
      "step 246, train_acc: 0.8695\n",
      "step 247, train_acc: 0.8696\n",
      "step 248, train_acc: 0.8696\n",
      "step 249, train_acc: 0.8697\n",
      "step 250, train_acc: 0.8698\n",
      "step 251, train_acc: 0.8700\n",
      "step 252, train_acc: 0.8700\n",
      "step 253, train_acc: 0.8702\n",
      "step 254, train_acc: 0.8702\n",
      "step 255, train_acc: 0.8704\n",
      "step 256, train_acc: 0.8706\n",
      "step 257, train_acc: 0.8705\n",
      "step 258, train_acc: 0.8705\n",
      "step 259, train_acc: 0.8705\n",
      "step 260, train_acc: 0.8705\n",
      "step 261, train_acc: 0.8704\n",
      "step 262, train_acc: 0.8705\n",
      "step 263, train_acc: 0.8706\n",
      "step 264, train_acc: 0.8705\n",
      "step 265, train_acc: 0.8705\n",
      "step 266, train_acc: 0.8705\n",
      "step 267, train_acc: 0.8705\n",
      "step 268, train_acc: 0.8704\n",
      "step 269, train_acc: 0.8705\n",
      "step 270, train_acc: 0.8705\n",
      "step 271, train_acc: 0.8705\n",
      "step 272, train_acc: 0.8705\n",
      "step 273, train_acc: 0.8703\n",
      "step 274, train_acc: 0.8703\n",
      "step 275, train_acc: 0.8703\n",
      "step 276, train_acc: 0.8704\n",
      "step 277, train_acc: 0.8705\n",
      "step 278, train_acc: 0.8706\n",
      "step 279, train_acc: 0.8707\n",
      "step 280, train_acc: 0.8707\n",
      "step 281, train_acc: 0.8708\n",
      "step 282, train_acc: 0.8708\n",
      "step 283, train_acc: 0.8708\n",
      "step 284, train_acc: 0.8708\n",
      "step 285, train_acc: 0.8708\n",
      "step 286, train_acc: 0.8708\n",
      "step 287, train_acc: 0.8707\n",
      "step 288, train_acc: 0.8708\n",
      "step 289, train_acc: 0.8709\n",
      "step 290, train_acc: 0.8710\n",
      "step 291, train_acc: 0.8709\n",
      "step 292, train_acc: 0.8708\n",
      "step 293, train_acc: 0.8708\n",
      "step 294, train_acc: 0.8709\n",
      "step 295, train_acc: 0.8710\n",
      "step 296, train_acc: 0.8710\n",
      "step 297, train_acc: 0.8711\n",
      "step 298, train_acc: 0.8712\n",
      "step 299, train_acc: 0.8714\n",
      "step 300, train_acc: 0.8715\n",
      "step 301, train_acc: 0.8716\n",
      "step 302, train_acc: 0.8714\n",
      "step 303, train_acc: 0.8714\n",
      "step 304, train_acc: 0.8715\n",
      "step 305, train_acc: 0.8713\n",
      "step 306, train_acc: 0.8712\n",
      "step 307, train_acc: 0.8712\n",
      "step 308, train_acc: 0.8712\n",
      "step 309, train_acc: 0.8712\n",
      "step 310, train_acc: 0.8712\n",
      "step 311, train_acc: 0.8711\n",
      "step 312, train_acc: 0.8709\n",
      "step 313, train_acc: 0.8710\n",
      "step 314, train_acc: 0.8709\n",
      "step 315, train_acc: 0.8709\n",
      "step 316, train_acc: 0.8711\n",
      "step 317, train_acc: 0.8712\n",
      "step 318, train_acc: 0.8711\n",
      "step 319, train_acc: 0.8712\n",
      "step 320, train_acc: 0.8711\n",
      "step 321, train_acc: 0.8712\n",
      "step 322, train_acc: 0.8714\n",
      "step 323, train_acc: 0.8715\n",
      "step 324, train_acc: 0.8716\n",
      "step 325, train_acc: 0.8716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dafcf4c6bad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ch5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DL/Dive-into-DL-Pytorch-IPynb/d2lzh_pytorch/utils.py\u001b[0m in \u001b[0;36mtrain_ch5\u001b[0;34m(net, train_iter, test_iter, batch_size, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr, epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ AlexNet跟LeNet结构类似，但使用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。\n",
    "+ 虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
