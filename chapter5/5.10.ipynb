{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量归一化\n",
    "本节我们介绍批量归一化$（batch normalization）$层，它能让较深的神经网络的训练变得更加容易。在$3.16$节（实战$Kaggle$比赛：预测房价）里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为$0$、标准差为$1$。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。\n",
    "\n",
    "通常来说，**数据标准化预处理对于浅层模型就足够有效了**。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，**即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化**。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。\n",
    "\n",
    "批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。**批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量归一化层\n",
    "对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对全连接层做批量归一化\n",
    "我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为$u$，权重参数和偏差参数分别为$W$和$b$，激活函数为$\\phi$。设批量归一化的运算符为$BN$。那么，使用批量归一化的全连接层的输出为\n",
    "<center>\n",
    "    $\\phi(BN(\\mathbf{x}))$\n",
    "</center>\n",
    "\n",
    "其中批量归一化输入$\\mathbf{x}$由仿射变换\n",
    "<center>\n",
    "    $\\mathbf{x}=\\mathbf{W}\\mathbf{u}+\\mathbf{b}$\n",
    "</center>\n",
    "\n",
    "得到。考虑一个由m个样本组成的小批量，仿射变换的输出为一个新的小批量$\\mathbf{\\beta}={\\mathbf{x}^{(1)},...,\\mathbf{x}^{(m)}}$。它们正是批量归一化层的输入。对于小批量$\\mathbf{\\beta}$中任意样本$\\mathbf{x}^{(i)}\\epsilon\\mathbb{R}^{d},1\\le i\\le m$，批量归一化层的输出同样是$d$维向量\n",
    "<center>\n",
    "    $ \\mathbf{y}^{(i)}=\\mathbf{B}\\mathbf{N}(\\mathbf{x}^{(i)}) $\n",
    "</center>\n",
    "\n",
    "并由以下几步求得。首先，对小批量$\\mathbf{\\beta}$求均值和方差\n",
    "<center>\n",
    "    $ \\mathbf{u}_{\\beta}\\leftarrow\\frac{1}{m}\\sum_{i=1}^m\\mathbf{x}^{(i)} $\n",
    "</center>\n",
    "<center>\n",
    "    $ \\mathbf{\\sigma}_{\\beta}^2\\leftarrow\\frac{1}{m}\\sum_{i=1}^m(\\mathbf{x}^{(i)}-\\mathbf{u}_{\\beta})^2 $\n",
    "</center>\n",
    "\n",
    "其中的平方计算是按元素求平方。接下来，使用按元素开放和按元素除法对$\\mathbf{x}^{(i)}$标准化：\n",
    "<center>\n",
    "    $ \\hat{\\mathbf{x}}^{(i)}\\leftarrow\\frac{\\mathbf{x}^{(i)}-\\mathbf{u}_{\\beta}}{\\sqrt{\\sigma_{\\mathbf{\\beta}}^2+\\epsilon}} $\n",
    "</center>\n",
    "\n",
    "这里$\\epsilon>0$是一个很小的常数，保证分母大于$0$。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸$(scale)$参数$\\mathbf{\\gamma}$和偏移$(shift)$参数$\\mathbf{\\beta}$。这两个参数和$\\mathbf{x}^{(i)}$形状相同，皆为$d$维向量。他们与$\\mathbf{x}^{(i)}$分别做按元素乘法（符号$\\bigodot$）和加法运算:\n",
    "<center>\n",
    "    $ \\mathbf{y}^{(i)}\\leftarrow\\mathbf{\\gamma}\\bigodot\\hat{\\mathbf{x}}^{(i)}+\\mathbf{\\beta} $\n",
    "</center>\n",
    "\n",
    "至此，我们得到了$\\mathbf{x}^{(i)}$的批量归一化的输出$\\mathbf{y}^{(i)}$。值得注意的是，可学习的拉伸和偏移参数保留了不对$\\hat{\\mathbf{x}}^{(i)}$做批量归一化的可能：此时只需学出$\\mathbf{\\gamma}=\\sqrt{\\mathbf{\\sigma}_{\\mathbf{\\beta}}^2+\\epsilon}$和$\\mathbf{\\beta}=\\mathbf{u}_{\\mathbf{\\beta}}$。我们可以对此这样理解：**如果批量归一化无益，理论上，学出的模型可以不使用批量归一化**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对卷积层做批量归一化\n",
    "对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且**每个通道都拥有独立的拉伸和偏移参数，并均为标量**。设小批量中有$m$个样本。在单个通道上，假设卷积计算输出的高和宽分别为$p$和$q$。我们需要对该通道中$m\\times p\\times q$个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中$m\\times p\\times q$个元素的均值和方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测时的批量归一化\n",
    "使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。**可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从零开始实现\n",
    "下面我们自己实现批量归一化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not is_training:\n",
    "        X_hat = (X - moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        if len(X.shape) == 2:\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean)**2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        X_hat = (X - mean)/torch.sqrt(var+eps)\n",
    "        moving_mean = momentum*moving_mean+(1.0-momentum)*mean\n",
    "        moving_var = momentum*moving_var+(1.0-momentum)*var\n",
    "    Y = gamma*X_hat+beta\n",
    "    return Y, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们自定义一个$BatchNorm$层。它保存参与求梯度和迭代的拉伸参数$gamma$和偏移参数$beta$，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。$BatchNorm$实例所需指定的$num\\_features$参数对于全连接层来说应为输出个数，对于卷积层来说则为输出通道数。该实例所需指定的$num\\_dims$参数对于全连接层和卷积层来说分别为$2$和$4$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        \n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training, \n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用批量归一化层的LeNet\n",
    "下面我们修改$5.5$节（卷积神经网络（$LeNet$））介绍的$LeNet$模型，从而应用批量归一化层。我们在所有的卷积层或全连接层之后、激活层之前加入批量归一化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, 5),\n",
    "    BatchNorm(6, num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(6, 16, 5),\n",
    "    BatchNorm(16, num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2),\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(16*4*4, 120),\n",
    "    BatchNorm(120, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    BatchNorm(84, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们训练修改后的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cpu\n",
      "step 1, train_acc: 0.1328\n",
      "step 2, train_acc: 0.1934\n",
      "step 3, train_acc: 0.2318\n",
      "step 4, train_acc: 0.3018\n",
      "step 5, train_acc: 0.3461\n",
      "step 6, train_acc: 0.3757\n",
      "step 7, train_acc: 0.4051\n",
      "step 8, train_acc: 0.4292\n",
      "step 9, train_acc: 0.4484\n",
      "step 10, train_acc: 0.4680\n",
      "step 11, train_acc: 0.4833\n",
      "step 12, train_acc: 0.5029\n",
      "step 13, train_acc: 0.5114\n",
      "step 14, train_acc: 0.5273\n",
      "step 15, train_acc: 0.5378\n",
      "step 16, train_acc: 0.5520\n",
      "step 17, train_acc: 0.5609\n",
      "step 18, train_acc: 0.5710\n",
      "step 19, train_acc: 0.5792\n",
      "step 20, train_acc: 0.5838\n",
      "step 21, train_acc: 0.5926\n",
      "step 22, train_acc: 0.6012\n",
      "step 23, train_acc: 0.6087\n",
      "step 24, train_acc: 0.6134\n",
      "step 25, train_acc: 0.6183\n",
      "step 26, train_acc: 0.6227\n",
      "step 27, train_acc: 0.6254\n",
      "step 28, train_acc: 0.6289\n",
      "step 29, train_acc: 0.6339\n",
      "step 30, train_acc: 0.6376\n",
      "step 31, train_acc: 0.6411\n",
      "step 32, train_acc: 0.6440\n",
      "step 33, train_acc: 0.6481\n",
      "step 34, train_acc: 0.6505\n",
      "step 35, train_acc: 0.6537\n",
      "step 36, train_acc: 0.6564\n",
      "step 37, train_acc: 0.6594\n",
      "step 38, train_acc: 0.6613\n",
      "step 39, train_acc: 0.6636\n",
      "step 40, train_acc: 0.6665\n",
      "step 41, train_acc: 0.6686\n",
      "step 42, train_acc: 0.6709\n",
      "step 43, train_acc: 0.6736\n",
      "step 44, train_acc: 0.6759\n",
      "step 45, train_acc: 0.6776\n",
      "step 46, train_acc: 0.6805\n",
      "step 47, train_acc: 0.6822\n",
      "step 48, train_acc: 0.6843\n",
      "step 49, train_acc: 0.6861\n",
      "step 50, train_acc: 0.6875\n",
      "step 51, train_acc: 0.6897\n",
      "step 52, train_acc: 0.6919\n",
      "step 53, train_acc: 0.6943\n",
      "step 54, train_acc: 0.6957\n",
      "step 55, train_acc: 0.6958\n",
      "step 56, train_acc: 0.6973\n",
      "step 57, train_acc: 0.6979\n",
      "step 58, train_acc: 0.6989\n",
      "step 59, train_acc: 0.7009\n",
      "step 60, train_acc: 0.7026\n",
      "step 61, train_acc: 0.7040\n",
      "step 62, train_acc: 0.7051\n",
      "step 63, train_acc: 0.7071\n",
      "step 64, train_acc: 0.7079\n",
      "step 65, train_acc: 0.7092\n",
      "step 66, train_acc: 0.7108\n",
      "step 67, train_acc: 0.7118\n",
      "step 68, train_acc: 0.7130\n",
      "step 69, train_acc: 0.7143\n",
      "step 70, train_acc: 0.7159\n",
      "step 71, train_acc: 0.7162\n",
      "step 72, train_acc: 0.7173\n",
      "step 73, train_acc: 0.7180\n",
      "step 74, train_acc: 0.7192\n",
      "step 75, train_acc: 0.7198\n",
      "step 76, train_acc: 0.7214\n",
      "step 77, train_acc: 0.7226\n",
      "step 78, train_acc: 0.7233\n",
      "step 79, train_acc: 0.7240\n",
      "step 80, train_acc: 0.7249\n",
      "step 81, train_acc: 0.7258\n",
      "step 82, train_acc: 0.7268\n",
      "step 83, train_acc: 0.7274\n",
      "step 84, train_acc: 0.7279\n",
      "step 85, train_acc: 0.7291\n",
      "step 86, train_acc: 0.7296\n",
      "step 87, train_acc: 0.7308\n",
      "step 88, train_acc: 0.7314\n",
      "step 89, train_acc: 0.7322\n",
      "step 90, train_acc: 0.7326\n",
      "step 91, train_acc: 0.7332\n",
      "step 92, train_acc: 0.7339\n",
      "step 93, train_acc: 0.7346\n",
      "step 94, train_acc: 0.7357\n",
      "step 95, train_acc: 0.7364\n",
      "step 96, train_acc: 0.7371\n",
      "step 97, train_acc: 0.7382\n",
      "step 98, train_acc: 0.7388\n",
      "step 99, train_acc: 0.7393\n",
      "step 100, train_acc: 0.7402\n",
      "step 101, train_acc: 0.7405\n",
      "step 102, train_acc: 0.7408\n",
      "step 103, train_acc: 0.7419\n",
      "step 104, train_acc: 0.7430\n",
      "step 105, train_acc: 0.7440\n",
      "step 106, train_acc: 0.7447\n",
      "step 107, train_acc: 0.7450\n",
      "step 108, train_acc: 0.7455\n",
      "step 109, train_acc: 0.7461\n",
      "step 110, train_acc: 0.7469\n",
      "step 111, train_acc: 0.7474\n",
      "step 112, train_acc: 0.7478\n",
      "step 113, train_acc: 0.7482\n",
      "step 114, train_acc: 0.7488\n",
      "step 115, train_acc: 0.7497\n",
      "step 116, train_acc: 0.7502\n",
      "step 117, train_acc: 0.7508\n",
      "step 118, train_acc: 0.7515\n",
      "step 119, train_acc: 0.7522\n",
      "step 120, train_acc: 0.7527\n",
      "step 121, train_acc: 0.7532\n",
      "step 122, train_acc: 0.7542\n",
      "step 123, train_acc: 0.7548\n",
      "step 124, train_acc: 0.7555\n",
      "step 125, train_acc: 0.7561\n",
      "step 126, train_acc: 0.7563\n",
      "step 127, train_acc: 0.7570\n",
      "step 128, train_acc: 0.7579\n",
      "step 129, train_acc: 0.7584\n",
      "step 130, train_acc: 0.7584\n",
      "step 131, train_acc: 0.7589\n",
      "step 132, train_acc: 0.7593\n",
      "step 133, train_acc: 0.7598\n",
      "step 134, train_acc: 0.7600\n",
      "step 135, train_acc: 0.7604\n",
      "step 136, train_acc: 0.7609\n",
      "step 137, train_acc: 0.7615\n",
      "step 138, train_acc: 0.7618\n",
      "step 139, train_acc: 0.7627\n",
      "step 140, train_acc: 0.7631\n",
      "step 141, train_acc: 0.7637\n",
      "step 142, train_acc: 0.7641\n",
      "step 143, train_acc: 0.7645\n",
      "step 144, train_acc: 0.7651\n",
      "step 145, train_acc: 0.7655\n",
      "step 146, train_acc: 0.7663\n",
      "step 147, train_acc: 0.7664\n",
      "step 148, train_acc: 0.7670\n",
      "step 149, train_acc: 0.7676\n",
      "step 150, train_acc: 0.7683\n",
      "step 151, train_acc: 0.7686\n",
      "step 152, train_acc: 0.7694\n",
      "step 153, train_acc: 0.7699\n",
      "step 154, train_acc: 0.7706\n",
      "step 155, train_acc: 0.7712\n",
      "step 156, train_acc: 0.7715\n",
      "step 157, train_acc: 0.7719\n",
      "step 158, train_acc: 0.7725\n",
      "step 159, train_acc: 0.7731\n",
      "step 160, train_acc: 0.7734\n",
      "step 161, train_acc: 0.7734\n",
      "step 162, train_acc: 0.7740\n",
      "step 163, train_acc: 0.7742\n",
      "step 164, train_acc: 0.7744\n",
      "step 165, train_acc: 0.7749\n",
      "step 166, train_acc: 0.7753\n",
      "step 167, train_acc: 0.7757\n",
      "step 168, train_acc: 0.7763\n",
      "step 169, train_acc: 0.7766\n",
      "step 170, train_acc: 0.7768\n",
      "step 171, train_acc: 0.7771\n",
      "step 172, train_acc: 0.7774\n",
      "step 173, train_acc: 0.7779\n",
      "step 174, train_acc: 0.7782\n",
      "step 175, train_acc: 0.7785\n",
      "step 176, train_acc: 0.7790\n",
      "step 177, train_acc: 0.7792\n",
      "step 178, train_acc: 0.7794\n",
      "step 179, train_acc: 0.7797\n",
      "step 180, train_acc: 0.7803\n",
      "step 181, train_acc: 0.7807\n",
      "step 182, train_acc: 0.7809\n",
      "step 183, train_acc: 0.7811\n",
      "step 184, train_acc: 0.7813\n",
      "step 185, train_acc: 0.7815\n",
      "step 186, train_acc: 0.7818\n",
      "step 187, train_acc: 0.7822\n",
      "step 188, train_acc: 0.7826\n",
      "step 189, train_acc: 0.7830\n",
      "step 190, train_acc: 0.7832\n",
      "step 191, train_acc: 0.7836\n",
      "step 192, train_acc: 0.7840\n",
      "step 193, train_acc: 0.7845\n",
      "step 194, train_acc: 0.7848\n",
      "step 195, train_acc: 0.7850\n",
      "step 196, train_acc: 0.7852\n",
      "step 197, train_acc: 0.7857\n",
      "step 198, train_acc: 0.7860\n",
      "step 199, train_acc: 0.7861\n",
      "step 200, train_acc: 0.7865\n",
      "step 201, train_acc: 0.7869\n",
      "step 202, train_acc: 0.7873\n",
      "step 203, train_acc: 0.7877\n",
      "step 204, train_acc: 0.7880\n",
      "step 205, train_acc: 0.7883\n",
      "step 206, train_acc: 0.7886\n",
      "step 207, train_acc: 0.7889\n",
      "step 208, train_acc: 0.7892\n",
      "step 209, train_acc: 0.7895\n",
      "step 210, train_acc: 0.7898\n",
      "step 211, train_acc: 0.7899\n",
      "step 212, train_acc: 0.7902\n",
      "step 213, train_acc: 0.7906\n",
      "step 214, train_acc: 0.7909\n",
      "step 215, train_acc: 0.7912\n",
      "step 216, train_acc: 0.7915\n",
      "step 217, train_acc: 0.7916\n",
      "step 218, train_acc: 0.7920\n",
      "step 219, train_acc: 0.7923\n",
      "step 220, train_acc: 0.7925\n",
      "step 221, train_acc: 0.7928\n",
      "step 222, train_acc: 0.7929\n",
      "step 223, train_acc: 0.7933\n",
      "step 224, train_acc: 0.7937\n",
      "step 225, train_acc: 0.7938\n",
      "step 226, train_acc: 0.7940\n",
      "step 227, train_acc: 0.7944\n",
      "step 228, train_acc: 0.7946\n",
      "step 229, train_acc: 0.7948\n",
      "step 230, train_acc: 0.7950\n",
      "step 231, train_acc: 0.7952\n",
      "step 232, train_acc: 0.7953\n",
      "step 233, train_acc: 0.7957\n",
      "step 234, train_acc: 0.7957\n",
      "step 235, train_acc: 0.7957\n",
      "epoch 1, loss 0.9803, train acc 0.796, test acc 0.815, time 28.6 sec\n",
      "step 1, train_acc: 0.8750\n",
      "step 2, train_acc: 0.8828\n",
      "step 3, train_acc: 0.8802\n",
      "step 4, train_acc: 0.8750\n",
      "step 5, train_acc: 0.8727\n",
      "step 6, train_acc: 0.8724\n",
      "step 7, train_acc: 0.8700\n",
      "step 8, train_acc: 0.8618\n",
      "step 9, train_acc: 0.8659\n",
      "step 10, train_acc: 0.8652\n",
      "step 11, train_acc: 0.8601\n",
      "step 12, train_acc: 0.8604\n",
      "step 13, train_acc: 0.8606\n",
      "step 14, train_acc: 0.8610\n",
      "step 15, train_acc: 0.8625\n",
      "step 16, train_acc: 0.8601\n",
      "step 17, train_acc: 0.8614\n",
      "step 18, train_acc: 0.8609\n",
      "step 19, train_acc: 0.8612\n",
      "step 20, train_acc: 0.8600\n",
      "step 21, train_acc: 0.8607\n",
      "step 22, train_acc: 0.8620\n",
      "step 23, train_acc: 0.8624\n",
      "step 24, train_acc: 0.8618\n",
      "step 25, train_acc: 0.8634\n",
      "step 26, train_acc: 0.8631\n",
      "step 27, train_acc: 0.8628\n",
      "step 28, train_acc: 0.8631\n",
      "step 29, train_acc: 0.8642\n",
      "step 30, train_acc: 0.8645\n",
      "step 31, train_acc: 0.8644\n",
      "step 32, train_acc: 0.8638\n",
      "step 33, train_acc: 0.8642\n",
      "step 34, train_acc: 0.8642\n",
      "step 35, train_acc: 0.8650\n",
      "step 36, train_acc: 0.8657\n",
      "step 37, train_acc: 0.8650\n",
      "step 38, train_acc: 0.8655\n",
      "step 39, train_acc: 0.8643\n",
      "step 40, train_acc: 0.8645\n",
      "step 41, train_acc: 0.8647\n",
      "step 42, train_acc: 0.8641\n",
      "step 43, train_acc: 0.8654\n",
      "step 44, train_acc: 0.8654\n",
      "step 45, train_acc: 0.8655\n",
      "step 46, train_acc: 0.8652\n",
      "step 47, train_acc: 0.8654\n",
      "step 48, train_acc: 0.8653\n",
      "step 49, train_acc: 0.8654\n",
      "step 50, train_acc: 0.8650\n",
      "step 51, train_acc: 0.8647\n",
      "step 52, train_acc: 0.8643\n",
      "step 53, train_acc: 0.8647\n",
      "step 54, train_acc: 0.8644\n",
      "step 55, train_acc: 0.8636\n",
      "step 56, train_acc: 0.8634\n",
      "step 57, train_acc: 0.8627\n",
      "step 58, train_acc: 0.8628\n",
      "step 59, train_acc: 0.8628\n",
      "step 60, train_acc: 0.8627\n",
      "step 61, train_acc: 0.8631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 62, train_acc: 0.8628\n",
      "step 63, train_acc: 0.8633\n",
      "step 64, train_acc: 0.8631\n",
      "step 65, train_acc: 0.8630\n",
      "step 66, train_acc: 0.8635\n",
      "step 67, train_acc: 0.8634\n",
      "step 68, train_acc: 0.8633\n",
      "step 69, train_acc: 0.8629\n",
      "step 70, train_acc: 0.8632\n",
      "step 71, train_acc: 0.8629\n",
      "step 72, train_acc: 0.8633\n",
      "step 73, train_acc: 0.8631\n",
      "step 74, train_acc: 0.8632\n",
      "step 75, train_acc: 0.8633\n",
      "step 76, train_acc: 0.8637\n",
      "step 77, train_acc: 0.8638\n",
      "step 78, train_acc: 0.8642\n",
      "step 79, train_acc: 0.8645\n",
      "step 80, train_acc: 0.8646\n",
      "step 81, train_acc: 0.8643\n",
      "step 82, train_acc: 0.8648\n",
      "step 83, train_acc: 0.8650\n",
      "step 84, train_acc: 0.8649\n",
      "step 85, train_acc: 0.8649\n",
      "step 86, train_acc: 0.8651\n",
      "step 87, train_acc: 0.8653\n",
      "step 88, train_acc: 0.8652\n",
      "step 89, train_acc: 0.8651\n",
      "step 90, train_acc: 0.8649\n",
      "step 91, train_acc: 0.8654\n",
      "step 92, train_acc: 0.8652\n",
      "step 93, train_acc: 0.8652\n",
      "step 94, train_acc: 0.8651\n",
      "step 95, train_acc: 0.8650\n",
      "step 96, train_acc: 0.8653\n",
      "step 97, train_acc: 0.8653\n",
      "step 98, train_acc: 0.8654\n",
      "step 99, train_acc: 0.8653\n",
      "step 100, train_acc: 0.8654\n",
      "step 101, train_acc: 0.8653\n",
      "step 102, train_acc: 0.8651\n",
      "step 103, train_acc: 0.8651\n",
      "step 104, train_acc: 0.8653\n",
      "step 105, train_acc: 0.8653\n",
      "step 106, train_acc: 0.8653\n",
      "step 107, train_acc: 0.8652\n",
      "step 108, train_acc: 0.8651\n",
      "step 109, train_acc: 0.8652\n",
      "step 110, train_acc: 0.8652\n",
      "step 111, train_acc: 0.8652\n",
      "step 112, train_acc: 0.8653\n",
      "step 113, train_acc: 0.8650\n",
      "step 114, train_acc: 0.8647\n",
      "step 115, train_acc: 0.8644\n",
      "step 116, train_acc: 0.8641\n",
      "step 117, train_acc: 0.8641\n",
      "step 118, train_acc: 0.8642\n",
      "step 119, train_acc: 0.8643\n",
      "step 120, train_acc: 0.8645\n",
      "step 121, train_acc: 0.8645\n",
      "step 122, train_acc: 0.8643\n",
      "step 123, train_acc: 0.8642\n",
      "step 124, train_acc: 0.8644\n",
      "step 125, train_acc: 0.8641\n",
      "step 126, train_acc: 0.8641\n",
      "step 127, train_acc: 0.8640\n",
      "step 128, train_acc: 0.8638\n",
      "step 129, train_acc: 0.8635\n",
      "step 130, train_acc: 0.8636\n",
      "step 131, train_acc: 0.8635\n",
      "step 132, train_acc: 0.8635\n",
      "step 133, train_acc: 0.8632\n",
      "step 134, train_acc: 0.8629\n",
      "step 135, train_acc: 0.8630\n",
      "step 136, train_acc: 0.8632\n",
      "step 137, train_acc: 0.8633\n",
      "step 138, train_acc: 0.8635\n",
      "step 139, train_acc: 0.8634\n",
      "step 140, train_acc: 0.8636\n",
      "step 141, train_acc: 0.8638\n",
      "step 142, train_acc: 0.8641\n",
      "step 143, train_acc: 0.8640\n",
      "step 144, train_acc: 0.8638\n",
      "step 145, train_acc: 0.8640\n",
      "step 146, train_acc: 0.8640\n",
      "step 147, train_acc: 0.8643\n",
      "step 148, train_acc: 0.8644\n",
      "step 149, train_acc: 0.8648\n",
      "step 150, train_acc: 0.8651\n",
      "step 151, train_acc: 0.8651\n",
      "step 152, train_acc: 0.8651\n",
      "step 153, train_acc: 0.8651\n",
      "step 154, train_acc: 0.8652\n",
      "step 155, train_acc: 0.8651\n",
      "step 156, train_acc: 0.8651\n",
      "step 157, train_acc: 0.8651\n",
      "step 158, train_acc: 0.8651\n",
      "step 159, train_acc: 0.8650\n",
      "step 160, train_acc: 0.8651\n",
      "step 161, train_acc: 0.8651\n",
      "step 162, train_acc: 0.8652\n",
      "step 163, train_acc: 0.8655\n",
      "step 164, train_acc: 0.8657\n",
      "step 165, train_acc: 0.8656\n",
      "step 166, train_acc: 0.8657\n",
      "step 167, train_acc: 0.8656\n",
      "step 168, train_acc: 0.8656\n",
      "step 169, train_acc: 0.8656\n",
      "step 170, train_acc: 0.8657\n",
      "step 171, train_acc: 0.8660\n",
      "step 172, train_acc: 0.8661\n",
      "step 173, train_acc: 0.8660\n",
      "step 174, train_acc: 0.8660\n",
      "step 175, train_acc: 0.8660\n",
      "step 176, train_acc: 0.8659\n",
      "step 177, train_acc: 0.8660\n",
      "step 178, train_acc: 0.8657\n",
      "step 179, train_acc: 0.8657\n",
      "step 180, train_acc: 0.8657\n",
      "step 181, train_acc: 0.8657\n",
      "step 182, train_acc: 0.8657\n",
      "step 183, train_acc: 0.8656\n",
      "step 184, train_acc: 0.8657\n",
      "step 185, train_acc: 0.8659\n",
      "step 186, train_acc: 0.8658\n",
      "step 187, train_acc: 0.8657\n",
      "step 188, train_acc: 0.8657\n",
      "step 189, train_acc: 0.8658\n",
      "step 190, train_acc: 0.8659\n",
      "step 191, train_acc: 0.8658\n",
      "step 192, train_acc: 0.8659\n",
      "step 193, train_acc: 0.8662\n",
      "step 194, train_acc: 0.8663\n",
      "step 195, train_acc: 0.8664\n",
      "step 196, train_acc: 0.8665\n",
      "step 197, train_acc: 0.8666\n",
      "step 198, train_acc: 0.8665\n",
      "step 199, train_acc: 0.8666\n",
      "step 200, train_acc: 0.8666\n",
      "step 201, train_acc: 0.8666\n",
      "step 202, train_acc: 0.8666\n",
      "step 203, train_acc: 0.8667\n",
      "step 204, train_acc: 0.8669\n",
      "step 205, train_acc: 0.8670\n",
      "step 206, train_acc: 0.8670\n",
      "step 207, train_acc: 0.8671\n",
      "step 208, train_acc: 0.8672\n",
      "step 209, train_acc: 0.8673\n",
      "step 210, train_acc: 0.8674\n",
      "step 211, train_acc: 0.8673\n",
      "step 212, train_acc: 0.8674\n",
      "step 213, train_acc: 0.8674\n",
      "step 214, train_acc: 0.8675\n",
      "step 215, train_acc: 0.8673\n",
      "step 216, train_acc: 0.8675\n",
      "step 217, train_acc: 0.8675\n",
      "step 218, train_acc: 0.8676\n",
      "step 219, train_acc: 0.8678\n",
      "step 220, train_acc: 0.8678\n",
      "step 221, train_acc: 0.8678\n",
      "step 222, train_acc: 0.8678\n",
      "step 223, train_acc: 0.8679\n",
      "step 224, train_acc: 0.8679\n",
      "step 225, train_acc: 0.8681\n",
      "step 226, train_acc: 0.8682\n",
      "step 227, train_acc: 0.8681\n",
      "step 228, train_acc: 0.8681\n",
      "step 229, train_acc: 0.8681\n",
      "step 230, train_acc: 0.8683\n",
      "step 231, train_acc: 0.8682\n",
      "step 232, train_acc: 0.8682\n",
      "step 233, train_acc: 0.8682\n",
      "step 234, train_acc: 0.8682\n",
      "step 235, train_acc: 0.8682\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ebcf48251613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ch5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DL/Dive-into-DL-Pytorch-IPynb/d2lzh_pytorch/utils.py\u001b[0m in \u001b[0;36mtrain_ch5\u001b[0;34m(net, train_iter, test_iter, batch_size, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step %d, train_acc: %.4f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n\u001b[1;32m    222\u001b[0m               % (epoch, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
      "\u001b[0;32m~/DL/Dive-into-DL-Pytorch-IPynb/d2lzh_pytorch/utils.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[0;34m(data_iter, net, device)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0macc_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m          \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 评估模式, 这会关闭dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们查看第一个批量归一化层学习到的拉伸参数$gamma$和偏移参数$beta$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-85851dde277c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "net[1].gamaa.view((-1,)), net[1].beta.view((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简洁实现\n",
    "与我们刚刚自己定义的$BatchNorm$类相比，$Pytorch$中$nn$模块定义的$BatchNorm1d$和$BatchNorm2d$类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的$num\\_features$参数值。下面我们用$PyTorch$实现使用批量归一化的$LeNet$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, 5),\n",
    "    nn.BatchNorm2d(6),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(6, 16, 5),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2),\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(16*4*4, 120),\n",
    "    nn.BatchNorm1d(120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.BatchNorm1d(84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。\n",
    "+ 对全连接层和卷积层做批量归一化的方法稍有不同。\n",
    "+ 批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。\n",
    "+ PyTorch提供了BatchNorm类方便使用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
