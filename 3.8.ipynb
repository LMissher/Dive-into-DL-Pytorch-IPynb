{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个小批量样本$\\mathbf{X}\\epsilon \\mathbb{R}^{n\\times d}$。假设多层感知机只有几个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出为$\\mathbf{H}$，有$ \\mathbf{H}\\epsilon \\mathbb{R}^{n\\times h} $。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$ \\mathbf{W}_h\\epsilon \\mathbb{R}^{d\\times h} $ 和 $ \\mathbf{b}_h\\epsilon\\mathbb{R}^{1\\times h} $，输出层的权重和偏差参数分别为$ \\mathbf{W}_o\\epsilon \\mathbb{R}^{h\\times q} $ 和 $ \\mathbf{b}_o\\epsilon\\mathbb{R}^{1\\times q} $。\n",
    "\n",
    "这样的设计下，其输出$ \\mathbf{O}\\epsilon\\mathbb{R}^{n\\times q} $的计算为\n",
    "<center>\n",
    "    $ \\mathbf{H} = \\mathbf{X}\\mathbf{W}_h+\\mathbf{b}_h $\n",
    "</center>\n",
    "<center>\n",
    "    $ \\mathbf{O} = \\mathbf{H}\\mathbf{W}_o+\\mathbf{b}_o $\n",
    "</center>\n",
    "\n",
    "将上面两个式子联立起来，可以得到：\n",
    "<center>\n",
    "    $ \\mathbf{O} = (\\mathbf{X}\\mathbf{W}_h+\\mathbf{b}_h)\\mathbf{W}_o+\\mathbf{b}_o = \\mathbf{X}\\mathbf{W}_h\\mathbf{W}_o+\\mathbf{b}_h\\mathbf{W}_o+\\mathbf{b}_o $\n",
    "</center>\n",
    "\n",
    "从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$ \\mathbf{W}_h\\mathbf{W}_o $, 偏差参数为 $ \\mathbf{b}_h\\mathbf{W}_o+\\mathbf{b}_o $。不难发现即使添加再多的隐藏层，以上设计依然只能与仅有输出层的单层神经网络等价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述问题的根源在于全连接层只是对数据做仿射变换$（affine transformation）$，而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数$（activation function）$。几个常用的激活函数$ReLU,tanh,sigmoid$等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。\n",
    "<center>\n",
    "    $ \\mathbf{H} = \\phi(\\mathbf{X}\\mathbf{W}_h+\\mathbf{b}_h) $\n",
    "</center>   \n",
    "<center>\n",
    "    $ \\mathbf{O}=\\mathbf{H}\\mathbf{W}_o+\\mathbf{b}_o $\n",
    "</center>\n",
    "\n",
    "其中$\\phi$表示激活函数。在分类问题中对输出$\\mathbf{O}$做$softmax$运算，并使用交叉熵损失函数。在线性回归中将输出$\\mathbf{O}$直接提供给平方损失函数即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
